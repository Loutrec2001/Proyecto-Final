{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport requests\nimport json\n\ndef base_de_datos():\n    df_japan = pd.read_csv(\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time\")\n    df_japan = df_japan[['time', 'latitude', 'longitude', 'depth', 'mag', 'place']]  # Campos válidos\n    df_japan = df_japan.dropna(subset=['place'])  # Eliminar filas con valores faltantes en la columna 'place'\n\n    estados = ['Alaska', 'California', 'Washington', 'Oregon']\n    df_japan = df_japan[df_japan['place'].str.contains('|'.join(estados), case=False)]\n    df_japan['localidad'] = df_japan['place']\n\n    df_eu = df_japan.reset_index(drop=True)  # Restablecer el índice a partir de 0\n    df_eu['time'] = pd.to_datetime(df_eu['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n    #Redondear las columnas \n    df_eu[['latitude','longitude','depth','mag']]=df_eu[['latitude','longitude','depth','mag']].round(1)\n\n\n    r = requests.get(\"https://www.jma.go.jp/bosai/quake/data/list.json\")  # Obtener datos de la API\n    r = r.text  # Texto del cuerpo de la respuesta\n    r = json.loads(r)  # Convertir a formato JSON\n\n    quakes = []  # Lista para almacenar los eventos sísmicos\n\n    for item in r:\n        if 'cod' in item:\n            vars = item['cod'][1:]\n            vars = vars.replace('+', ',')\n            vars = vars.replace('-', ',')\n            vars = vars.replace('/', '')\n            vars = vars.split(',')\n\n            if len(vars) >= 3:\n                lat = vars[0]\n                lon = vars[1]\n                dept = vars[2]  # Convertir profundidad a 'km'\n\n                quake = {'time': item['at'], 'latitude': lat, 'longitude': lon, 'depth': dept, 'mag': item['mag'], 'place': item['en_anm'], 'localidad': item['en_anm']}\n                quakes.append(quake)\n    \n    df_japan = pd.DataFrame(quakes)\n    #convertimos a float la columna \"depth\"\n    df_japan['depth'] = df_japan['depth'].astype('float64')\n    #dividimos por mil para llevar la unidad de medida a KM para mantener la misma en todos los datasets\n    df_japan['depth'] = (df_japan['depth'] / 1000)\n    df_japan['time'] = pd.to_datetime(df_japan['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n    #Redondear las columnas \n    df_japan[['latitude','longitude','depth','mag']]=df_japan[['latitude','longitude','depth','mag']].round(1)\n\n    # Leer el archivo CSV desde la URL\n    df_csv = pd.read_csv('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time')\n\n    # Eliminar filas con valores nulos en la columna 'place'\n    df_csv = df_csv.dropna(subset=['place'])\n\n    # Filtrar los registros de México\n    df_mexico = df_csv[df_csv['place'].str.contains('Mexico')]\n\n    # Seleccionar las columnas deseadas\n    df_mexico = df_mexico[['time', 'latitude', 'longitude', 'depth', 'mag', 'place']]\n    df_mexico['localidad'] = df_mexico['place']\n    df_mexico['time'] = pd.to_datetime(df_mexico['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n    #Redondear las columnas \n    df_mexico[['latitude','longitude','depth','mag']]=df_mexico[['latitude','longitude','depth','mag']].round(1)\n\n    # Agregar columna 'country' a df_mexico con valor 'mexico'\n    df_mexico['country'] = 'mexico'\n\n    # Agregar columna 'country' a df_eu con valor 'EE.UU.'\n    df_eu['country'] = 'ee.uu'\n\n    # Agregar columna 'country' a df_japan con valor 'Japan'\n    df_japan['country'] = 'japan'\n\n    # Unir los DataFrames utilizando pd.concat()\n    df_eu_japan = pd.concat([df_japan, df_eu, df_mexico], axis=0, ignore_index=True)\n    df_eu_japan = df_eu_japan.drop(columns=['place'])\n    return df_eu_japan"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"e4e07dc1-8c70-494d-94cf-afe9b88a535d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_eu_jap_mex = base_de_datos()\ndf_eu_jap_mex"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"db8a1887-c54a-440d-8023-1445aba30b2f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>depth</th>\n      <th>mag</th>\n      <th>localidad</th>\n      <th>country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-07-18 20:01:00</td>\n      <td>36.9</td>\n      <td>140.9</td>\n      <td>60.0</td>\n      <td>3.6</td>\n      <td>Off the Coast of Fukushima Prefecture</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-07-18 07:33:00</td>\n      <td>37.4</td>\n      <td>141.5</td>\n      <td>40.0</td>\n      <td>3.7</td>\n      <td>Off the Coast of Fukushima Prefecture</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-07-17 10:22:00</td>\n      <td>43.0</td>\n      <td>142.3</td>\n      <td>10.0</td>\n      <td>2.4</td>\n      <td>Southern Kamikawa Region, Hokkaido</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-07-17 09:37:00</td>\n      <td>42.8</td>\n      <td>143.4</td>\n      <td>110.0</td>\n      <td>4.4</td>\n      <td>Central Tokachi Region, Hokkaido</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-07-17 09:37:00</td>\n      <td>42.8</td>\n      <td>143.4</td>\n      <td>110.0</td>\n      <td>4.4</td>\n      <td>Central Tokachi Region, Hokkaido</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5239</th>\n      <td>2023-06-19 02:32:26</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>6.6</td>\n      <td>1.9</td>\n      <td>57 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5240</th>\n      <td>2023-06-19 02:23:39</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>6.2</td>\n      <td>2.6</td>\n      <td>58 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5241</th>\n      <td>2023-06-19 01:55:40</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>6.6</td>\n      <td>2.1</td>\n      <td>58 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5242</th>\n      <td>2023-06-19 01:38:23</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>7.8</td>\n      <td>2.1</td>\n      <td>57 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5243</th>\n      <td>2023-06-18 23:22:49</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>7.4</td>\n      <td>1.9</td>\n      <td>57 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n  </tbody>\n</table>\n<p>5244 rows × 7 columns</p>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>depth</th>\n","      <th>mag</th>\n","      <th>localidad</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023-07-18 20:01:00</td>\n","      <td>36.9</td>\n","      <td>140.9</td>\n","      <td>60.0</td>\n","      <td>3.6</td>\n","      <td>Off the Coast of Fukushima Prefecture</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023-07-18 07:33:00</td>\n","      <td>37.4</td>\n","      <td>141.5</td>\n","      <td>40.0</td>\n","      <td>3.7</td>\n","      <td>Off the Coast of Fukushima Prefecture</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023-07-17 10:22:00</td>\n","      <td>43.0</td>\n","      <td>142.3</td>\n","      <td>10.0</td>\n","      <td>2.4</td>\n","      <td>Southern Kamikawa Region, Hokkaido</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023-07-17 09:37:00</td>\n","      <td>42.8</td>\n","      <td>143.4</td>\n","      <td>110.0</td>\n","      <td>4.4</td>\n","      <td>Central Tokachi Region, Hokkaido</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023-07-17 09:37:00</td>\n","      <td>42.8</td>\n","      <td>143.4</td>\n","      <td>110.0</td>\n","      <td>4.4</td>\n","      <td>Central Tokachi Region, Hokkaido</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5239</th>\n","      <td>2023-06-19 02:32:26</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>6.6</td>\n","      <td>1.9</td>\n","      <td>57 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5240</th>\n","      <td>2023-06-19 02:23:39</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>6.2</td>\n","      <td>2.6</td>\n","      <td>58 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5241</th>\n","      <td>2023-06-19 01:55:40</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>6.6</td>\n","      <td>2.1</td>\n","      <td>58 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5242</th>\n","      <td>2023-06-19 01:38:23</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>7.8</td>\n","      <td>2.1</td>\n","      <td>57 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5243</th>\n","      <td>2023-06-18 23:22:49</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>7.4</td>\n","      <td>1.9</td>\n","      <td>57 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5244 rows × 7 columns</p>\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Delete row 1\ndf_eu_jap_mex = df_eu_jap_mex.drop(index=0, inplace=False)\ndf_eu_jap_mex"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"cb7a98d5-d5b1-4963-b4e4-f40e11c5ccd5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>depth</th>\n      <th>mag</th>\n      <th>localidad</th>\n      <th>country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2023-07-18 07:33:00</td>\n      <td>37.4</td>\n      <td>141.5</td>\n      <td>40.0</td>\n      <td>3.7</td>\n      <td>Off the Coast of Fukushima Prefecture</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-07-17 10:22:00</td>\n      <td>43.0</td>\n      <td>142.3</td>\n      <td>10.0</td>\n      <td>2.4</td>\n      <td>Southern Kamikawa Region, Hokkaido</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-07-17 09:37:00</td>\n      <td>42.8</td>\n      <td>143.4</td>\n      <td>110.0</td>\n      <td>4.4</td>\n      <td>Central Tokachi Region, Hokkaido</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-07-17 09:37:00</td>\n      <td>42.8</td>\n      <td>143.4</td>\n      <td>110.0</td>\n      <td>4.4</td>\n      <td>Central Tokachi Region, Hokkaido</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2023-07-17 08:03:00</td>\n      <td>34.2</td>\n      <td>135.2</td>\n      <td>10.0</td>\n      <td>2.6</td>\n      <td>Northern Wakayama Prefecture</td>\n      <td>japan</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5239</th>\n      <td>2023-06-19 02:32:26</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>6.6</td>\n      <td>1.9</td>\n      <td>57 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5240</th>\n      <td>2023-06-19 02:23:39</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>6.2</td>\n      <td>2.6</td>\n      <td>58 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5241</th>\n      <td>2023-06-19 01:55:40</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>6.6</td>\n      <td>2.1</td>\n      <td>58 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5242</th>\n      <td>2023-06-19 01:38:23</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>7.8</td>\n      <td>2.1</td>\n      <td>57 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n    <tr>\n      <th>5243</th>\n      <td>2023-06-18 23:22:49</td>\n      <td>31.7</td>\n      <td>-104.3</td>\n      <td>7.4</td>\n      <td>1.9</td>\n      <td>57 km S of Whites City, New Mexico</td>\n      <td>mexico</td>\n    </tr>\n  </tbody>\n</table>\n<p>5243 rows × 7 columns</p>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>depth</th>\n","      <th>mag</th>\n","      <th>localidad</th>\n","      <th>country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>2023-07-18 07:33:00</td>\n","      <td>37.4</td>\n","      <td>141.5</td>\n","      <td>40.0</td>\n","      <td>3.7</td>\n","      <td>Off the Coast of Fukushima Prefecture</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023-07-17 10:22:00</td>\n","      <td>43.0</td>\n","      <td>142.3</td>\n","      <td>10.0</td>\n","      <td>2.4</td>\n","      <td>Southern Kamikawa Region, Hokkaido</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023-07-17 09:37:00</td>\n","      <td>42.8</td>\n","      <td>143.4</td>\n","      <td>110.0</td>\n","      <td>4.4</td>\n","      <td>Central Tokachi Region, Hokkaido</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023-07-17 09:37:00</td>\n","      <td>42.8</td>\n","      <td>143.4</td>\n","      <td>110.0</td>\n","      <td>4.4</td>\n","      <td>Central Tokachi Region, Hokkaido</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2023-07-17 08:03:00</td>\n","      <td>34.2</td>\n","      <td>135.2</td>\n","      <td>10.0</td>\n","      <td>2.6</td>\n","      <td>Northern Wakayama Prefecture</td>\n","      <td>japan</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5239</th>\n","      <td>2023-06-19 02:32:26</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>6.6</td>\n","      <td>1.9</td>\n","      <td>57 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5240</th>\n","      <td>2023-06-19 02:23:39</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>6.2</td>\n","      <td>2.6</td>\n","      <td>58 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5241</th>\n","      <td>2023-06-19 01:55:40</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>6.6</td>\n","      <td>2.1</td>\n","      <td>58 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5242</th>\n","      <td>2023-06-19 01:38:23</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>7.8</td>\n","      <td>2.1</td>\n","      <td>57 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","    <tr>\n","      <th>5243</th>\n","      <td>2023-06-18 23:22:49</td>\n","      <td>31.7</td>\n","      <td>-104.3</td>\n","      <td>7.4</td>\n","      <td>1.9</td>\n","      <td>57 km S of Whites City, New Mexico</td>\n","      <td>mexico</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5243 rows × 7 columns</p>\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport requests\nimport json\n\n# Función para obtener el último sismo\ndef get_last_quake():\n    global df_eu_jap_mex\n    countries = {'ee.uu': 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time',\n                 'japan': 'https://www.jma.go.jp/bosai/quake/data/list.json',\n                'mexico': 'https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time'}\n    \n    for country in countries:\n        quake = get_quake_by_country(country, countries[country])\n        df_eu_jap_mex = save(quake)\n        df_eu_jap_mex_tr = transformaciones(df_eu_jap_mex)\n    return df_eu_jap_mex_tr\n\n# Función para obtener el sismo por país\ndef get_quake_by_country(country, url):\n    if country == 'ee.uu':\n        df = pd.read_csv(url)\n        df = df.dropna(subset=['place'])\n        df = df[['time', 'latitude', 'longitude', 'depth', 'mag', 'place']]\n        estados = ['Alaska', 'California', 'Washington', 'Oregon']\n        df = df[df['place'].str.contains('|'.join(estados), case=False)]\n        df['localidad'] = df['place']\n        df['country'] = 'ee.uu'\n        df = df.drop(columns=['place'])\n        quake = df.head(1)\n        # Formato correcto a la columna de fechas\n        quake['time'] = pd.to_datetime(quake['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n        #Redondear las columnas \n        quake[['latitude','longitude','depth','mag']]=quake[['latitude','longitude','depth','mag']].round(1)\n        return quake\n    \n    elif country == 'japan':\n        r = requests.get(url)\n        r = r.text\n        r = json.loads(r)\n        r = r[0]\n        vars = r['cod'][1:]\n        vars = vars.replace('+', ',')\n        vars = vars.replace('-', ',')\n        vars = vars.replace('/', '')\n        vars = vars.split(',')\n        lat = vars[0]\n        lon = vars[1]\n        dept = vars[2]\n        quake = {'time': r['at'], 'latitude': lat, 'longitude': lon, 'depth': dept, 'mag': r['mag'],'country': 'japon', 'localidad': r['en_anm']}\n        quake = pd.DataFrame([quake])\n        #convertimos a float la columna \"depth\"\n        quake['depth'] = quake['depth'].astype('float64')\n        #dividimos por mil para llevar la unidad de medida a KM para mantener la misma en todos los datasets\n        quake['depth'] = (quake['depth'] / 1000)\n        # Formato correcto a la columna de fechas\n        quake['time'] = pd.to_datetime(quake['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n        #Redondear las columnas \n        quake[['latitude','longitude','depth','mag']]=quake[['latitude','longitude','depth','mag']].round(1)\n        return quake\n    \n    elif country == 'mexico':\n        # Leer el archivo CSV desde la URL\n        df_csv = pd.read_csv('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time')\n        # Eliminar filas con valores nulos en la columna 'place'\n        df_csv = df_csv.dropna(subset=['place'])\n        # Filtrar los registros de México\n        df_mexico = df_csv[df_csv['place'].str.contains('Mexico')]\n        # Seleccionar las columnas deseadas\n        df_mexico = df_mexico[['time', 'latitude', 'longitude', 'depth', 'mag', 'place']]\n        df_mexico['localidad'] = df_mexico['place']\n        df_mexico['country'] = 'mexico'\n        df_mexico = df_mexico.drop(columns=['place'])\n        quake = df_mexico.head(1)\n        # Formato correcto a la columna de fechas\n        quake['time'] = pd.to_datetime(quake['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n        #Redondear las columnas \n        quake[['latitude','longitude','depth','mag']]=quake[['latitude','longitude','depth','mag']].round(1)\n        return quake\n\n# Función para guardar el sismo en el DataFrame\ndef save(quake):\n    global df_eu_jap_mex\n    df_eu_jap_mex = pd.concat([df_eu_jap_mex, quake], axis=0, ignore_index=True)\n    # Drop duplicates\n    df_eu_jap_mex = df_eu_jap_mex.drop_duplicates()\n    return df_eu_jap_mex\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"c81c3af7-28ea-4aef-8029-407d39a2b3b5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def transformaciones(df):\n\n    # Convertir la columna \"Magnitud\" y \"prrofundidad\" a tipo float si es necesario\n    df['mag'] = df['mag'].astype(float)\n    df['depth'] = df['depth'].astype(float)  # Convertir a tipo float\n\n    # Aplicar la función a cada fila de la columna \"Profundidad\" y crear la nueva columna \"peligrosidad\"\n    df['Clasificación Profundidad'] = df['depth'].apply(clasificar_profundidad)\n\n    # Aplicar la función a cada fila de la columna \"Magnitud\" y crear la nueva columna \"peligrosidad\"\n    df['Clasificación Magnitud'] = df['mag'].apply(determinar_peligrosidad)\n    \n    df = df.drop_duplicates().reset_index(drop=True)\n    \n    # Renombrar columnas\n    df = df.rename(columns={'country': 'país', 'depth': 'profundidad', 'latitude': 'latitud', 'localidad': 'localidad', 'longitude': 'longitud', 'mag': 'magnitud', 'time': 'fecha'})\n\n    # Ordenar columnas\n    column_order = ['fecha', 'longitud', 'latitud', 'país', 'localidad' ,'profundidad','Clasificación Profundidad','magnitud','Clasificación Magnitud']\n    df = df[column_order] \n\n    #Todo el dataset a minuscula\n    df=df.applymap(lambda x: x.lower() if isinstance(x,str) else x)\n\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"5b51cebd-9b36-46da-a810-30640f7f1465","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Definir la función para clasificar la peligrosidad y profundidad \ndef determinar_peligrosidad(magnitud):\n    if magnitud < 2.0:\n        return 'Micro Magnitud'\n    elif magnitud < 3.0:\n        return 'Menor Magnitud'\n    elif magnitud < 4.0:\n        return 'Ligera Magnitud'\n    elif magnitud < 5.0:\n        return 'Moderada Magnitud'\n    elif magnitud < 6.0:\n        return 'Fuerte Magnitud'\n    elif magnitud < 7.0:\n        return 'Mayor Magnitud'\n    elif magnitud < 8.0:\n        return 'Gran Magnitud'\n    else:\n        return 'Magnitud Épica'\n    \ndef clasificar_profundidad(profundidad):\n\n    if profundidad <= 10:\n        return \"Superficial\"\n    elif profundidad > 10 and profundidad <= 30:\n        return\"Media\"\n    elif profundidad > 30:\n        return\"Profundo\"\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"0989e784-d4e2-422d-af39-a3a550bf4574","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\n# Definir las columnas del DataFrame vacío\ncolumnas = ['fecha', 'longitud', 'latitud', 'país', 'localidad', 'profundidad',\n            'Clasificación Profundidad', 'magnitud', 'Clasificación Magnitud']\n\n# Crear el DataFrame vacío\ndf_vacio = pd.DataFrame(columns=columnas)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"a47e0b0a-b83e-4c1f-865f-52bfc86d1094","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def main():\n    global df_vacio\n    df_referencia = get_last_quake()\n\n    # Ultimo dato de japon\n    r = requests.get(\"https://www.jma.go.jp/bosai/quake/data/list.json\")\n    r = r.text\n    r = json.loads(r)\n    r = r[0]\n    vars = r['cod'][1:]\n    vars = vars.replace('+', ',')\n    vars = vars.replace('-', ',')\n    vars = vars.replace('/', '')\n    vars = vars.split(',')\n    lat = vars[0]\n    lon = vars[1]\n    dept = vars[2]\n    quake = {'time': r['at'], 'latitude': lat, 'longitude': lon, 'depth': dept, 'mag': r['mag'], 'localidad': r['en_anm'], 'country': 'japon'}\n    Japon = pd.DataFrame([quake])\n    #convertimos a float la columna \"depth\"\n    Japon['depth'] = Japon['depth'].astype('float64')\n    #dividimos por mil para llevar la unidad de medida a KM para mantener la misma en todos los datasets\n    Japon['depth'] = (Japon['depth'] / 1000)\n    # Formato correcto a la columna de fechas\n    Japon['time'] = pd.to_datetime(Japon['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n    #Redondear las columnas \n    Japon[['latitude','longitude','depth','mag']]=Japon[['latitude','longitude','depth','mag']].round(1)\n\n    # Ultimo dato de estados unidos\n    df = pd.read_csv('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time')\n    df = df.dropna(subset=['place'])\n    df = df[['time', 'latitude', 'longitude', 'depth', 'mag', 'place']]\n    estados = ['Alaska', 'California', 'Washington', 'Oregon']\n    df = df[df['place'].str.contains('|'.join(estados), case=False)]\n    df['localidad'] = df['place']\n    df['country'] = 'ee.uu'\n    df = df.drop(columns=['place'])\n    eeuu = df.head(1)\n    # Formato correcto a la columna de fechas\n    eeuu['time'] = pd.to_datetime(eeuu['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n    #Redondear las columnas \n    eeuu[['latitude','longitude','depth','mag']]=eeuu[['latitude','longitude','depth','mag']].round(1)\n    \n\n    # Ultimo dato de mexico\n    df = pd.read_csv('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&orderby=time')\n    # Eliminar filas con valores nulos en la columna 'place'\n    df = df.dropna(subset=['place'])\n    # Filtrar los registros de México\n    df_mexico = df[df['place'].str.contains('Mexico')]\n    # Seleccionar las columnas deseadas\n    df_mexico = df_mexico[['time', 'latitude', 'longitude', 'depth', 'mag', 'place']]\n    df_mexico['localidad'] = df_mexico['place']\n    df_mexico['country'] = 'mexico'\n    df_mexico = df_mexico.drop(columns=['place'])\n    mexico = df_mexico.head(1)\n    # Formato correcto a la columna de fechas\n    mexico['time'] = pd.to_datetime(mexico['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n    #Redondear las columnas \n    mexico[['latitude','longitude','depth','mag']]=mexico[['latitude','longitude','depth','mag']].round(1)\n\n    # Concatenar los tres conjuntos de datos\n    df_combinado = pd.concat([Japon, eeuu, mexico], ignore_index=True)\n    df_combinado = transformaciones(df_combinado)\n\n    df_referencia = pd.concat([df_referencia, df_combinado])\n    # Aplico el modelo de Machine Learning\n    df_referencia=etiquetado(df_referencia)\n    # Actualizar el DataFrame de referencia con las filas faltantes\n    df_referencia = df_referencia.drop_duplicates()\n    # Aplicar la función 'enviar_notificacion' a la primera fila si no está presente en 'df_vacio'\n    if df_vacio.empty:\n        enviar_notificacion(df_referencia.iloc[-1])\n        df_vacio = pd.concat([df_vacio, df_referencia.iloc[[-1]]], ignore_index=True)\n\n        #Verificar si la última fila de 'df_referencia' está presente en 'df_vacio'\n    elif not df_vacio.empty:\n        fila_presente = df_vacio.iloc[-1].equals(df_referencia.iloc[-1])\n\n        if not fila_presente:\n            enviar_notificacion(df_referencia.iloc[-1])\n            # Agregar la primera fila a 'df_vacio' si no está presente\n            df_vacio = pd.concat([df_vacio, df_referencia.iloc[[-1]]], ignore_index=True)\n        elif fila_presente:\n            print(\"La fila ya está presente en df_vacio. No se enviará la notificación.\")\n        \n    return df_referencia"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"c3b4253a-fb37-4320-b963-dd87736dbade","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%pip install python-telegram-bot"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"b30b5bd1-1f5b-46c4-8b59-2536eea0853e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nCollecting python-telegram-bot\n  Using cached python_telegram_bot-20.4-py3-none-any.whl (549 kB)\nCollecting httpx~=0.24.1\n  Using cached httpx-0.24.1-py3-none-any.whl (75 kB)\nCollecting sniffio\n  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.9/site-packages (from httpx~=0.24.1->python-telegram-bot) (3.3)\nCollecting httpcore<0.18.0,>=0.15.0\n  Using cached httpcore-0.17.3-py3-none-any.whl (74 kB)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.9/site-packages (from httpx~=0.24.1->python-telegram-bot) (2021.10.8)\nCollecting anyio<5.0,>=3.0\n  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\nCollecting h11<0.15,>=0.13\n  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\nCollecting exceptiongroup\n  Using cached exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\nInstalling collected packages: sniffio, exceptiongroup, h11, anyio, httpcore, httpx, python-telegram-bot\nSuccessfully installed anyio-3.7.1 exceptiongroup-1.1.2 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 python-telegram-bot-20.4 sniffio-1.3.0\nPython interpreter will be restarted.\n"]}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nfrom telegram import Bot\n\ndef enviar_notificacion(record):\n    # Configurar el token del bot de Telegram\n    telegram_token = '6150807593:AAELUrhl2gy62lscxmxGipLSg6pw_l4brMw'\n    id_  ='-994506464'\n   \n    # Extraer detalles del terremoto\n    Pais = record['país']\n    Magnitud = record['magnitud']\n    Latitud = record['latitud']\n    Longitud = record['longitud']\n    Profundidad = record['profundidad']\n    Etiquetas = record['etiquetas']\n    Ubicacion = record['localidad']\n    if Pais == 'usa':\n        emoji_pais = \"🇺🇸\"\n    elif Pais == 'mexico':\n        emoji_pais = \"🇲🇽\"\n    elif Pais == 'japon':\n        emoji_pais = \"🇯🇵\"\n    else:\n        emoji_pais = \"\"\n    \n    etiqueta = record['etiquetas']\n    if etiqueta == 'leve':\n        mensaje = \"🌱🌞 Te recomendamos revisar las conexiones de luz, gas y agua para asegurarte de su integridad.\"\n    elif etiqueta == 'alto':\n        mensaje = f\"⚠️🔥 Si es posible, evacua el edificio y dirígete a un área abierta lejos de objetos que puedan caer.\"\n    elif etiqueta == 'medio':\n        mensaje = \"🏠 Para tu seguridad, es recomendable resguardarte debajo de una mesa o la cama hasta que el sismo termine.\"\n    else:\n        mensaje = \"Se ha detectado un sismo. Mantén la calma y sigue las indicaciones de seguridad local.\"\n\n    # Mensaje de notificación\n    message_text = f\"🚨 ¡ALERTA DE SISMO EN {Pais.upper()}!\\n\\n\" \\\n                f\"Ubicacion {Ubicacion.upper()} \\n\" \\\n                f\"🔺Nivel: {etiqueta.capitalize()}🔺\\n\" \\\n                f\"{mensaje}\\n\\n\" \\\n                f\"Por favor, mantente a salvo y sigue las indicaciones de las autoridades locales.\"\n    send_text = 'https://api.telegram.org/bot' + telegram_token + '/sendMessage?chat_id=' + id_ + '&parse_mode=Markdown&text=' + message_text\n\n    response = requests.get(send_text)\n\n    print('Notificación de Telegram enviada')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"7da29ac9-accb-40f9-89fc-2d64ba93ab94","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef etiquetado(df):\n  # Separar los campos que se utilizarán para ejecutar el modelo\n    X = df[['profundidad', 'magnitud']]\n\n    # Seleccionar el modelo K-means\n    kmeans = KMeans(n_clusters=4, random_state=0)\n\n    # Entrenar el modelo\n    kmeans.fit(X)\n\n    # Obtener las etiquetas\n    etiquetas = kmeans.labels_\n\n    # Asignar etiquetas a las categorías 'leve', 'medio' y 'alto'\n    df['etiquetas'] = np.where(etiquetas == 0, 'leve',\n                                np.where(etiquetas == 1, 'medio',\n                                        np.where(etiquetas == 2, 'alto', '')))\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"bb37a1e3-a233-419c-a4f6-57365e4a4db9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import time\n\ndef run_main_every_60_seconds():\n    while True:\n        df_referencia = main()\n        # Puedes realizar cualquier operación adicional con df_referencia aquí\n        time.sleep(60)\n\nrun_main_every_60_seconds()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"nuid":"eaa79f33-027e-4544-bf9a-6bcc160563c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stderr","text":["<command-1348473818977254>:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  quake['time'] = pd.to_datetime(quake['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n<command-1348473818977254>:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  quake[['latitude','longitude','depth','mag']]=quake[['latitude','longitude','depth','mag']].round(1)\n<command-1348473818977254>:75: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  quake['time'] = pd.to_datetime(quake['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n<command-1348473818977254>:77: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  quake[['latitude','longitude','depth','mag']]=quake[['latitude','longitude','depth','mag']].round(1)\n<command-1348473818977258>:40: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  eeuu['time'] = pd.to_datetime(eeuu['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n<command-1348473818977258>:42: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  eeuu[['latitude','longitude','depth','mag']]=eeuu[['latitude','longitude','depth','mag']].round(1)\n<command-1348473818977258>:58: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  mexico['time'] = pd.to_datetime(mexico['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n<command-1348473818977258>:60: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  mexico[['latitude','longitude','depth','mag']]=mexico[['latitude','longitude','depth','mag']].round(1)\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["Notificación de Telegram enviada\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.11.3","nbconvert_exporter":"python","file_extension":".py"},"orig_nbformat":4,"application/vnd.databricks.v1+notebook":{"notebookName":"Optimización ML","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
